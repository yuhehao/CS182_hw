%!TEX program = xelatex
\documentclass[10pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{titlesec}
\usepackage{xcolor}
%\usepackage[shortlabels]{enumitem}
\usepackage{enumerate}
\usepackage{bm}
\usepackage{tikz}
\usepackage{listings}
\usetikzlibrary{arrows}
\usepackage{subfigure}
\usepackage{graphicx,booktabs,multirow}
\usepackage[a4paper]{geometry}
\usepackage{upquote}
\usepackage{float}
\usepackage{pdfpages}

\geometry{verbose,tmargin=2cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}
\geometry{verbose,tmargin=2cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}
\lstset{language=Matlab}
\lstset{breaklines}


\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}



\begin{document}

\date{\today}
\title{Introduction to Machine Learning, Spring 2022 \\
Homework 2\\
\small (Due Friday, Apr. 8 at 11:59pm (CST))}
\maketitle


\section{Problem1}
Given a set of data $\{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}$, where $y_i \in \{0, 1\}$. We want to conduct a binary classification, and the decision boundary is $\beta_0 + x^T\beta = 0$. When $\beta_0 + x^T\beta > 0$, the sample will be classified as 1, and 0 otherwise. 

\begin{enumerate}

\item[(a)]
Define a function which enables to map the range of an arbitrary linear function to the range of a probability {\color{red} [2 points]}\\

\item[(b)]
Derive the posterior probability of $P(y_i=1 | x_i)$ and $P(y_i=0 | x_i)$ {\color{red} [3 points]}\\

\item[(c)]
Write the log-likelihood for N observations, which means:
$$l(\theta) = logP(Y|X) = \sum_{i=1}^Nlog(P(y_i|x_i))$$
(Using the expression of $P(y_i|x_i)$ in (b) and eliminate redundant items) {\color{red} [5 points]}\\

\end{enumerate}
\newpage
\section{Problem2}

\begin{enumerate}
    \item[(a)] 
    Given a random variable $X$ and its probability distribution is shown in Table 1. Now, we sample 8 times and get the results $\{3,1,3,0,3,1,2,3\}$. Please derive the MLE estimate for $\theta$($0 < \theta < \frac{1}{2}$). {\color{red} [4 points]} \\
    \begin{table}[t]
      \caption{probability distribution for $X$}
      \label{table1}
      \centering
      \begin{tabular}{c|c|c|c|c}
        \hline
        $X$ & 0 & 1 & 2 & 3 \\ \hline
        $P$ & $\theta^{2}$ & $2\theta(1-\theta)$ & $\theta^{2}$ & $1-2\theta$ \\
        \hline
      \end{tabular}
    \end{table}
    
    \item[(b)]
    Now we discuss Bayesian inference in coin flipping. Let's denote the number of heads and the total number of trials by $N_1$ and $N$, respectively. Please derive the MAP estimate based on the following prior:
    \begin{align*}
        p(\theta)=\left\{\begin{array}{ll}
        0.5 & \text { if } \theta=0.5 \\
        0.5 & \text { if } \theta=0.3 \\
        0   & \text { otherwise,}
        \end{array}\right.
    \end{align*}
    which believes the coin is fair, or is slightly biased towards tails. {\color{red} [4 points]} \\
    
    \item[(c)]
    Suppose the true parameter is $\theta = 0.31$. Please compare the prior in (b) with the Beta prior distribution(You can review this part in Lecture 07). Which prior leads to a better estimate when $N$ is small? Which prior leads to a better estimate when N is large? {\color{red} [2 points]} \\

\end{enumerate}
\newpage
\section{Problem3}
According to the following Fig.~\ref{fig2}, answer the following questions:
\begin{enumerate}
    \item[(a)]
    use the D-separation to discus whether the following statements are true or not: 
    \begin{itemize}
        \item[(1)] Given $x_4$, $ \{x_1, x_2\} $ and $ \{x_6, x_7\} $ are conditionally independent.{\color{red} [1(reason)+1(conclusion) points]}
        
		\item[(2)] Given $x_6$, $ x_3 $ and $ x_2 $ are conditionally independent.{\color{red} [1(reason)+1(conclusion) points]}
    \end{itemize}

	\item[(b)] if all the nodes are observed and boolean variables, please complete the process of learning the parameter $ \theta_{x_6|i,j} $ by using \textbf{MLE}, where $ \theta_{x_6|i,j} = p(x_6 = 1\mid x_3 = i, x_4 = j), i,j \in \{ 0, 1\}$.{\color{red} [6 points]}
	
	\begin{figure}[h!] 
		\centering
		\includegraphics[width=2.5in]{3.png}
		\label{fig2}
		\caption{The Bayesian network for questions $3$.}
	\end{figure}
\end{enumerate}


\end{document}
